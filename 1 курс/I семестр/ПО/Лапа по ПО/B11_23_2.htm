<html xmlns:v="urn:schemas-microsoft-com:vml"
xmlns:o="urn:schemas-microsoft-com:office:office"
xmlns:w="urn:schemas-microsoft-com:office:word"
xmlns="http://www.w3.org/TR/REC-html40">

<head>
<meta http-equiv=Content-Type content="text/html; charset=windows-1251">
<meta name=ProgId content=FrontPage.Editor.Document>
<meta name=Generator content="Microsoft FrontPage 4.0">
<meta name=Originator content="Microsoft Word 9">
<link rel=File-List href="./B11_23_2.files/filelist.xml">
<title>В вероятностно - статистической теории связи (именно эта теория положила
начало теории информации) информация определяется как противоположность
неопределенности, как уменьшаемая неопределенность (энтропия).</title>
<!--[if gte mso 9]><xml>
 <o:DocumentProperties>
  <o:Author>Томилин В.В.</o:Author>
  <o:Template>Normal</o:Template>
  <o:LastAuthor>Красиков Андрей</o:LastAuthor>
  <o:Revision>2</o:Revision>
  <o:TotalTime>7</o:TotalTime>
  <o:LastPrinted>1601-01-01T00:00:00Z</o:LastPrinted>
  <o:Created>2001-12-10T17:54:00Z</o:Created>
  <o:LastSaved>2001-12-10T17:54:00Z</o:LastSaved>
  <o:Pages>2</o:Pages>
  <o:Words>520</o:Words>
  <o:Characters>2964</o:Characters>
  <o:Company>TSTU</o:Company>
  <o:Lines>24</o:Lines>
  <o:Paragraphs>5</o:Paragraphs>
  <o:CharactersWithSpaces>3640</o:CharactersWithSpaces>
  <o:Version>9.2812</o:Version>
 </o:DocumentProperties>
</xml><![endif]--><!--[if gte mso 9]><xml>
 <w:WordDocument>
  <w:HyphenationZone>21</w:HyphenationZone>
  <w:DisplayHorizontalDrawingGridEvery>0</w:DisplayHorizontalDrawingGridEvery>
  <w:DisplayVerticalDrawingGridEvery>0</w:DisplayVerticalDrawingGridEvery>
  <w:UseMarginsForDrawingGridOrigin/>
  <w:Compatibility>
   <w:UsePrinterMetrics/>
   <w:WW6BorderRules/>
   <w:FootnoteLayoutLikeWW8/>
   <w:ShapeLayoutLikeWW8/>
   <w:AlignTablesRowByRow/>
   <w:ForgetLastTabAlignment/>
   <w:LayoutRawTableWidth/>
   <w:LayoutTableRowsApart/>
  </w:Compatibility>
 </w:WordDocument>
</xml><![endif]-->
<style>
<!--
 /* Style Definitions */
p.MsoNormal, li.MsoNormal, div.MsoNormal
	{mso-style-parent:"";
	margin:0cm;
	margin-bottom:.0001pt;
	mso-pagination:widow-orphan;
	font-size:11.0pt;
	mso-bidi-font-size:10.0pt;
	font-family:"Times New Roman";
	mso-fareast-font-family:"Times New Roman";}
p.MsoTitle, li.MsoTitle, div.MsoTitle
	{margin:0cm;
	margin-bottom:.0001pt;
	text-align:center;
	mso-pagination:widow-orphan;
	font-size:12.0pt;
	mso-bidi-font-size:10.0pt;
	font-family:"Times New Roman";
	mso-fareast-font-family:"Times New Roman";
	font-weight:bold;
	mso-bidi-font-weight:normal;}
@page Section1
	{size:21.0cm 842.0pt;
	margin:1.0cm 1.0cm 1.0cm 2.0cm;
	mso-header-margin:36.0pt;
	mso-footer-margin:36.0pt;
	mso-paper-source:0;}
div.Section1
	{page:Section1;}
 /* List Definitions */
@list l0
	{mso-list-id:646324098;
	mso-list-type:simple;
	mso-list-template-ids:-93688770;}
@list l0:level1
	{mso-level-tab-stop:none;
	mso-level-number-position:left;
	mso-level-legacy:yes;
	mso-level-legacy-indent:14.15pt;
	mso-level-legacy-space:0cm;
	margin-left:14.15pt;
	text-indent:-14.15pt;}
@list l0:level1 lfo2
	{mso-level-numbering:continue;
	mso-level-tab-stop:none;
	mso-level-number-position:left;
	mso-level-legacy:yes;
	mso-level-legacy-indent:14.15pt;
	mso-level-legacy-space:0cm;
	margin-left:14.15pt;
	text-indent:-14.15pt;}
@list l0:level1 lfo3
	{mso-level-numbering:continue;
	mso-level-tab-stop:none;
	mso-level-number-position:left;
	mso-level-legacy:yes;
	mso-level-legacy-indent:14.15pt;
	mso-level-legacy-space:0cm;
	margin-left:14.15pt;
	text-indent:-14.15pt;}
@list l0:level1 lfo4
	{mso-level-numbering:continue;
	mso-level-tab-stop:none;
	mso-level-number-position:left;
	mso-level-legacy:yes;
	mso-level-legacy-indent:14.15pt;
	mso-level-legacy-space:0cm;
	margin-left:14.15pt;
	text-indent:-14.15pt;}
ol
	{margin-bottom:0cm;}
ul
	{margin-bottom:0cm;}
-->
</style>
<!--[if gte mso 9]><xml>
 <o:shapedefaults v:ext="edit" spidmax="2050"/>
</xml><![endif]--><!--[if gte mso 9]><xml>
 <o:shapelayout v:ext="edit">
  <o:idmap v:ext="edit" data="1"/>
 </o:shapelayout></xml><![endif]-->
</head>

<body lang=RU style='tab-interval:35.4pt'>

<div class=Section1>

<h2 align="center" style="text-align:center;text-indent:36.0pt;
line-height:normal"><a href="../../bil11.htm"><font color="#FF0000" size="5">Назад</font></a></h2>

<p class=MsoTitle>
&nbsp;
</p>

<p class=MsoTitle>БИЛЕТ № 23</p>

<p class=MsoNormal align=center style='text-align:center'><b style='mso-bidi-font-weight:
normal'><span style='font-size:12.0pt;mso-bidi-font-size:10.0pt'><![if !supportEmptyParas]>&nbsp;<![endif]><o:p></o:p></span></b></p>

<p class=MsoNormal style='text-align:justify'><b style='mso-bidi-font-weight:
normal'><span style='font-size:12.0pt;mso-bidi-font-size:10.0pt'>2. Информация.
Вероятностный подход к измерению количества информации.<o:p></o:p></span></b></p>

<p class=MsoNormal style='text-align:justify'><span style='font-size:12.0pt;
mso-bidi-font-size:10.0pt'><![if !supportEmptyParas]>&nbsp;<![endif]><o:p></o:p></span></p>

<p class=MsoNormal style='text-align:justify'><span style='font-size:12.0pt;
mso-bidi-font-size:10.0pt'>Понятие информации - одно из самых фундаментальных в
современной науке. Наряду с такими понятиями, как вещество, энергия,
пространство и время, оно составляет основу современной научной картины мира.<o:p></o:p></span></p>

<p class=MsoNormal style='text-align:justify'><span style='font-size:12.0pt;
mso-bidi-font-size:10.0pt'><![if !supportEmptyParas]>&nbsp;<![endif]><o:p></o:p></span></p>

<p class=MsoNormal style='text-align:justify'><span style='font-size:12.0pt;
mso-bidi-font-size:10.0pt'>Под информацией <b style='mso-bidi-font-weight:normal'><i
style='mso-bidi-font-style:normal'>в теории информации</i></b> понимают не
любые сведения, а лишь те , которые снимают<span style="mso-spacerun: yes"> 
</span>полностью<span style="mso-spacerun: yes">  </span>или уменьшают существующую
до их получения неопределенность. Информация - это снятая неопределенность.
(Клод Шеннон)<o:p></o:p></span></p>

<p class=MsoNormal style='text-align:justify'><span style='font-size:12.0pt;
mso-bidi-font-size:10.0pt'><![if !supportEmptyParas]>&nbsp;<![endif]><o:p></o:p></span></p>

<p class=MsoNormal style='text-align:justify'><span style='font-size:12.0pt;
mso-bidi-font-size:10.0pt'>В теории информации <b style='mso-bidi-font-weight:
normal'>количеством информации</b> называют числовую характеристику сигнала,
которая не зависит от его формы и содержания и характеризует неопределенность,
которая исчезает после получения сообщения в виде данного сигнала. В этом
случае количество информации зависит от <b style='mso-bidi-font-weight:normal'><i
style='mso-bidi-font-style:normal'>вероятности получения</i></b> сообщения о
том или ином событии.<o:p></o:p></span></p>

<p class=MsoNormal style='text-align:justify'><span style='font-size:12.0pt;
mso-bidi-font-size:10.0pt'><![if !supportEmptyParas]>&nbsp;<![endif]><o:p></o:p></span></p>

<p class=MsoNormal style='text-align:justify'><span style='font-size:12.0pt;
mso-bidi-font-size:10.0pt'>Считается, что сообщение несет 1 бит информации,
если после его получения неопределенность уменьшилась ровно в два раза.<o:p></o:p></span></p>

<p class=MsoNormal style='text-align:justify'><span style='font-size:12.0pt;
mso-bidi-font-size:10.0pt'><![if !supportEmptyParas]>&nbsp;<![endif]><o:p></o:p></span></p>

<p class=MsoNormal style='text-align:justify'><span style='font-size:12.0pt;
mso-bidi-font-size:10.0pt'>ПРИМЕРЫ<o:p></o:p></span></p>

<p class=MsoNormal style='text-align:justify'><span style='font-size:12.0pt;
mso-bidi-font-size:10.0pt;mso-bidi-font-style:italic'>Сообщение о том, как
упала монета после броска - “орлом” или “решкой”, несет один бит информации.<o:p></o:p></span></p>

<p class=MsoNormal style='text-align:justify'><span style='font-size:12.0pt;
mso-bidi-font-size:10.0pt;mso-bidi-font-style:italic'>Сообщение о том, что на
светофоре красный сигнал, несет в себе информации больше, чем бит, так как
исходных вариантов три.</span><i style='mso-bidi-font-style:normal'><o:p></o:p></i></p>

<p class=MsoNormal style='text-align:justify'><span style='font-size:12.0pt;
mso-bidi-font-size:10.0pt'><![if !supportEmptyParas]>&nbsp;<![endif]><o:p></o:p></span></p>

<p class=MsoNormal style='text-align:justify'><span style='font-size:12.0pt;
mso-bidi-font-size:10.0pt'>Для абсолютно достоверного события (событие
обязательно произойдет, поэтому его вероятность<span style="mso-spacerun:
yes">  </span>равна 1) количество вероятности в сообщении о нем равно 0. Чем
невероятнее событие, тем большую информацию о нем несет сообщение.<o:p></o:p></span></p>

<p class=MsoNormal style='text-align:justify'><span style='font-size:12.0pt;
mso-bidi-font-size:10.0pt'>Итак, количество информации в сообщении о каком-то
событии зависит от вероятности свершения данного события. <o:p></o:p></span></p>

<p class=MsoNormal style='text-align:justify'><span style='font-size:12.0pt;
mso-bidi-font-size:10.0pt'><![if !supportEmptyParas]>&nbsp;<![endif]><o:p></o:p></span></p>

<p class=MsoNormal style='text-align:justify'><span style='font-size:12.0pt;
mso-bidi-font-size:10.0pt'>Научный подход к оценке сообщений был предложен еще
в 1928 году Р.Хартли. Расчетная формула имеет вид: <o:p></o:p></span></p>

<p class=MsoNormal align=center style='text-align:center'><b style='mso-bidi-font-weight:
normal'><span lang=EN-US style='font-size:12.0pt;mso-bidi-font-size:10.0pt;
mso-ansi-language:EN-US'>I = log<sub>2 </sub>N<span style="mso-spacerun:
yes">   </span></span></b><b style='mso-bidi-font-weight:normal'><span
style='font-size:12.0pt;mso-bidi-font-size:10.0pt'>или</span></b><b
style='mso-bidi-font-weight:normal'><span lang=EN-US style='font-size:12.0pt;
mso-bidi-font-size:10.0pt;mso-ansi-language:EN-US'><span style="mso-spacerun:
yes">   </span>2<sup>I</sup> = N,</span></b><span lang=EN-US style='font-size:
12.0pt;mso-bidi-font-size:10.0pt;mso-ansi-language:EN-US'> <o:p></o:p></span></p>

<p class=MsoNormal style='text-align:justify'><span style='font-size:12.0pt;
mso-bidi-font-size:10.0pt'>где <span style='mso-tab-count:1'>      </span>N -
количество <i style='mso-bidi-font-style:normal'>равновероятных</i> событий
(число возможных выборов), <o:p></o:p></span></p>

<p class=MsoNormal style='text-align:justify;text-indent:35.4pt'><span
style='font-size:12.0pt;mso-bidi-font-size:10.0pt'>I - количество информации. <o:p></o:p></span></p>

<p class=MsoNormal style='text-align:justify'><span style='font-size:12.0pt;
mso-bidi-font-size:10.0pt'>Если N = 2 (выбор из двух возможностей), то I = 1
бит.<o:p></o:p></span></p>

<p class=MsoNormal style='text-align:justify'><span style='font-size:12.0pt;
mso-bidi-font-size:10.0pt'><![if !supportEmptyParas]>&nbsp;<![endif]><o:p></o:p></span></p>

<p class=MsoNormal style='text-align:justify'><span style='font-size:12.0pt;
mso-bidi-font-size:10.0pt'>Бит выбран в качестве единицы количества информации
потому, что принято считать, что двумя двоичными словами исходной длины k или
словом длины 2k можно передать в 2 раза больше информации, чем одним исходным
словом. Число возможных равновероятных выборов при этом увеличивается в 2<sup>k</sup>
раз, тогда как количество информации (I) удваивается.<o:p></o:p></span></p>

<p class=MsoNormal style='text-align:justify'><span style='font-size:12.0pt;
mso-bidi-font-size:10.0pt'><![if !supportEmptyParas]>&nbsp;<![endif]><o:p></o:p></span></p>

<p class=MsoNormal style='text-align:justify'><span style='font-size:12.0pt;
mso-bidi-font-size:10.0pt'>Иногда формула Хартли записывается иначе. Так как
наступление каждого из N возможных событий имеет одинаковую вероятность<span
style="mso-spacerun: yes">  </span><b style='mso-bidi-font-weight:normal'>p = 1
/ N</b>, то <b style='mso-bidi-font-weight:normal'>N = 1 / p</b> и формула
имеет вид<o:p></o:p></span></p>

<p class=MsoNormal align=center style='text-align:center'><b style='mso-bidi-font-weight:
normal'><span lang=EN-US style='font-size:12.0pt;mso-bidi-font-size:10.0pt;
mso-ansi-language:EN-US'>I = log<sub>2 </sub>(1/p) = - log<sub>2 </sub>p</span></b><span
lang=EN-US style='font-size:12.0pt;mso-bidi-font-size:10.0pt;mso-ansi-language:
EN-US'><o:p></o:p></span></p>

<p class=MsoNormal style='text-align:justify'><span lang=EN-US
style='font-size:12.0pt;mso-bidi-font-size:10.0pt;mso-ansi-language:EN-US'><![if !supportEmptyParas]>&nbsp;<![endif]><o:p></o:p></span></p>

<p class=MsoNormal style='text-align:justify'><span style='font-size:12.0pt;
mso-bidi-font-size:10.0pt'>Более общий подход к вычислению количества
информации в сообщении об одном из N, но уже неравновероятных событий был
предложен К.Шенноном в 1948 году.<o:p></o:p></span></p>

<p class=MsoNormal style='text-align:justify'><span style='font-size:12.0pt;
mso-bidi-font-size:10.0pt'><![if !supportEmptyParas]>&nbsp;<![endif]><o:p></o:p></span></p>

<p class=MsoNormal style='text-align:justify'><span style='font-size:12.0pt;
mso-bidi-font-size:10.0pt'>Пусть имеется строка текста, содержащая тысячу букв.
Буква “о” в тексте встречается примерно 90 раз, буква ”р” ~ 40 раз, буква “ф” ~
2 раза, буква “а” ~ 200 раз. Поделив 200 на 1000, мы получим величину 0.2,
которая представляет собой среднюю частоту, с которой в рассматриваемом тексте
встречается буква “а”. Вероятность появления буквы “а” в тексте (p<sub>a</sub>)
можем считать приблизительно равной 0.2. Аналогично, p<sub>р </sub>= 0.04, p<sub>ф</sub>
= 0.002, р<sub>о</sub> = 0.09. <o:p></o:p></span></p>

<p class=MsoNormal style='text-align:justify'><span style='font-size:12.0pt;
mso-bidi-font-size:10.0pt'><![if !supportEmptyParas]>&nbsp;<![endif]><o:p></o:p></span></p>

<p class=MsoNormal style='text-align:justify'><span style='font-size:12.0pt;
mso-bidi-font-size:10.0pt'>Двоичный логарифм от величины 0.2 определяет то
количество информации, которое переносит одна-единственная буква “а” в
рассматриваемом тексте. <o:p></o:p></span></p>

<p class=MsoNormal style='text-align:justify'><span style='font-size:12.0pt;
mso-bidi-font-size:10.0pt'>Тогда количество информации, переносимой одной
буквой равно <o:p></o:p></span></p>

<p class=MsoNormal align=center style='text-align:center'><b style='mso-bidi-font-weight:
normal'><span lang=EN-US style='font-size:12.0pt;mso-bidi-font-size:10.0pt;
mso-ansi-language:EN-US'>h<sub>i</sub> = log<sub>2 </sub>1/p<sub>i</sub> = -
log<sub>2</sub> p<sub>i</sub>,</span></b><span lang=EN-US style='font-size:
12.0pt;mso-bidi-font-size:10.0pt;mso-ansi-language:EN-US'><o:p></o:p></span></p>

<p class=MsoNormal style='text-align:justify'><span style='font-size:12.0pt;
mso-bidi-font-size:10.0pt'>где p<sub>i </sub>- вероятность появления в
сообщении i-го символа алфавита.<o:p></o:p></span></p>

<p class=MsoNormal style='text-align:justify'><span style='font-size:12.0pt;
mso-bidi-font-size:10.0pt'><![if !supportEmptyParas]>&nbsp;<![endif]><o:p></o:p></span></p>

<p class=MsoNormal style='text-align:justify'><span style='font-size:12.0pt;
mso-bidi-font-size:10.0pt'>Удобнее в качестве меры количества информации
пользоваться не значением h<sub>i , </sub>а средним значением количества
информации, приходящейся на один символ алфавита<sub><o:p></o:p></sub></span></p>

<p class=MsoNormal align=center style='text-align:center'><b style='mso-bidi-font-weight:
normal'><span lang=EN-US style='font-size:12.0pt;mso-bidi-font-size:10.0pt;
mso-ansi-language:EN-US'>H = </span></b><b style='mso-bidi-font-weight:normal'><span
style='font-size:12.0pt;mso-bidi-font-size:10.0pt;font-family:Symbol'>S</span></b><b
style='mso-bidi-font-weight:normal'><span lang=EN-US style='font-size:12.0pt;
mso-bidi-font-size:10.0pt;mso-ansi-language:EN-US'> p<sub>i</sub> h<sub>i </sub>=
- </span></b><b style='mso-bidi-font-weight:normal'><span style='font-size:
12.0pt;mso-bidi-font-size:10.0pt;font-family:Symbol'>S </span></b><b
style='mso-bidi-font-weight:normal'><span lang=EN-US style='font-size:12.0pt;
mso-bidi-font-size:10.0pt;mso-ansi-language:EN-US'>p<sub>i </sub>log<sub>2</sub>
p<sub>i</sub></span></b><sub><span lang=EN-US style='font-size:12.0pt;
mso-bidi-font-size:10.0pt;mso-ansi-language:EN-US'><o:p></o:p></span></sub></p>

<p class=MsoNormal style='text-align:justify'><span style='font-size:12.0pt;
mso-bidi-font-size:10.0pt'>Значение Н достигает максимума при равновероятных
событиях, то есть при равенстве всех p<sub>i <o:p></o:p></sub></span></p>

<p class=MsoNormal align=center style='text-align:center'><b style='mso-bidi-font-weight:
normal'><span style='font-size:12.0pt;mso-bidi-font-size:10.0pt'>p<sub>i </sub>=
1 / N.</span></b><span style='font-size:12.0pt;mso-bidi-font-size:10.0pt'> <o:p></o:p></span></p>

<p class=MsoNormal style='text-align:justify'><span style='font-size:12.0pt;
mso-bidi-font-size:10.0pt'>В этом случае формула Шеннона превращается в формулу
Хартли.<o:p></o:p></span></p>

</div>

</body>

</html>
